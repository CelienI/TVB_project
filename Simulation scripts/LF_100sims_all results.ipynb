{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heard-chassis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab nbagg\n",
    "from tvb.simulator.lab import *\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tvb.simulator.lab import *\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy \n",
    "import scipy.fftpack\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy \n",
    "import scipy.fftpack\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy import signal\n",
    "from scipy.integrate import simps\n",
    "from fooof import FOOOF\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sapphire-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#depressed brain \n",
    "Qvals= [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]\n",
    "Qvals = np.array(Qvals)\n",
    "oscilator = models.WilsonCowan(Q = Qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beneficial-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscilator = models.WilsonCowan(c_ee = np.repeat(14.05, 76), c_ei = np.repeat(12.44, 76), c_ie = np.repeat(16.76, 76),\n",
    "                              c_ii = np.repeat(2.0, 76), tau_e = np.repeat(16.07, 76), tau_i = np.repeat(33.71, 76),\n",
    "                              a_e = np.repeat(1.3, 76), b_e = np.repeat(4.0, 76), c_e = np.repeat(1.0, 76),\n",
    "                              a_i = np.repeat(1.95, 76), b_i = np.repeat(4.76, 76), c_i = np.repeat(1.0, 76),\n",
    "                              r_e = np.repeat(1.0, 76), r_i = np.repeat(1.0, 76), k_e = np.repeat(1.0, 76),\n",
    "                              k_i = np.repeat(1.0, 76), P = np.repeat(2.22, 76), Q = Qvals, theta_e = np.repeat(0.0, 76),\n",
    "                              theta_i = np.repeat(0.0, 76), alpha_e = np.repeat(1.0, 76), alpha_i = np.repeat(1.0, 76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "least-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqn_t = equations.PulseTrain()\n",
    "eqn_t.parameters['onset'] = 0.0\n",
    "eqn_t.parameters['T'] = 1000.0\n",
    "eqn_t.parameters['tau'] = 1.0\n",
    "eqn_t.parameters[\"amp\"] = 0.035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subjective-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure stimulus spatial pattern: LF\n",
    "weighting = numpy.zeros((76, ))\n",
    "weighting[[18]] = 5   #LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  File 'hemispheres' not found in ZIP.\n"
     ]
    }
   ],
   "source": [
    "white_matter = connectivity.Connectivity.from_file()\n",
    "white_matter.speed = numpy.array([3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thorough-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_matter_coupling = coupling.Linear(a=numpy.array([0.0039]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus = patterns.StimuliRegion(\n",
    "    temporal=eqn_t,\n",
    "    connectivity=white_matter,\n",
    "    weight=weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prepared-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise some Monitors with period in physical time\n",
    "mon_raw = monitors.Raw(period = 0.9765625)\n",
    "mon_tavg = monitors.TemporalAverage(period=1)\n",
    "\n",
    "#Bundle them\n",
    "what_to_watch = (mon_raw, mon_tavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polar-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important variables of dataset\n",
    "sample_period = 0.0009765625 #seconds         \n",
    "sf = 1/sample_period #sampling frequency \n",
    "sample_rate = 1024\n",
    "\n",
    "#sns.set(font_scale=1.2)\n",
    "dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apart-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['areas', 'centres', 'cortical', 'hemispheres', 'orientations', 'region_labels', 'tract_lengths', 'weights']>\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/celie/OneDrive - ugentbe/NOG NODIG/2de master/Masterproef/Wilson Cowan/DATA\")\n",
    "#import connectivity file to know which region corresponds to which index in the matrices\n",
    "filename = \"Connectivity.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    regions_list = list(f['region_labels'])\n",
    "\n",
    "regions = np.array(regions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proprietary-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(data):\n",
    "    array_sum = np.sum(data)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "    if array_has_nan == True:\n",
    "        NAN = True\n",
    "    else:\n",
    "        NAN = False\n",
    "    return NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "maritime-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powercalc(data, low, high, sf, win):\n",
    "    freqs, psd = signal.welch(data, sf, nperseg=win)\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    total_power = simps(psd, dx=freq_res)\n",
    "    idx= np.logical_and(freqs >= low, freqs <= high)\n",
    "    power = simps(psd[idx], dx=freq_res)\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "given-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "check1\n",
      "check2\n",
      "2\n",
      "check1\n",
      "check2\n",
      "3\n",
      "check1\n",
      "check2\n",
      "4\n",
      "check1\n",
      "check2\n",
      "5\n",
      "check1\n",
      "check2\n",
      "6\n",
      "check1\n",
      "check2\n",
      "7\n",
      "check1\n",
      "check2\n",
      "8\n",
      "check1\n",
      "check2\n",
      "9\n",
      "check1\n",
      "check2\n",
      "10\n",
      "check1\n",
      "check2\n",
      "11\n",
      "check1\n",
      "check2\n",
      "12\n",
      "check1\n",
      "check2\n",
      "13\n",
      "check1\n",
      "check2\n",
      "14\n",
      "check1\n",
      "check2\n",
      "15\n",
      "check1\n",
      "check2\n",
      "16\n",
      "check1\n",
      "check2\n",
      "17\n",
      "check1\n",
      "check2\n",
      "18\n",
      "check1\n",
      "check2\n",
      "19\n",
      "check1\n",
      "check2\n",
      "20\n",
      "check1\n",
      "check2\n",
      "21\n",
      "check1\n",
      "check2\n",
      "22\n",
      "check1\n",
      "check2\n",
      "23\n",
      "check1\n",
      "check2\n",
      "24\n",
      "check1\n",
      "check2\n",
      "25\n",
      "check1\n",
      "check2\n",
      "26\n",
      "check1\n",
      "check2\n",
      "27\n",
      "check1\n",
      "check2\n",
      "28\n",
      "check1\n",
      "check2\n",
      "29\n",
      "check1\n",
      "check2\n",
      "30\n",
      "check1\n",
      "check2\n",
      "31\n",
      "check1\n",
      "check2\n",
      "32\n",
      "check1\n",
      "check2\n",
      "33\n",
      "check1\n",
      "check2\n",
      "34\n",
      "check1\n",
      "check2\n",
      "35\n",
      "check1\n",
      "check2\n",
      "36\n",
      "check1\n",
      "check2\n",
      "37\n",
      "check1\n",
      "check2\n",
      "38\n",
      "check1\n",
      "check2\n",
      "39\n",
      "check1\n",
      "check2\n",
      "40\n",
      "check1\n",
      "check2\n",
      "41\n",
      "check1\n",
      "check2\n",
      "42\n",
      "check1\n",
      "check2\n",
      "43\n",
      "check1\n",
      "check2\n",
      "44\n",
      "check1\n",
      "check2\n",
      "45\n",
      "check1\n",
      "check2\n",
      "46\n",
      "check1\n",
      "check2\n",
      "47\n",
      "check1\n",
      "check2\n",
      "48\n",
      "check1\n",
      "check2\n",
      "49\n",
      "check1\n",
      "check2\n",
      "50\n",
      "check1\n",
      "check2\n",
      "51\n",
      "check1\n",
      "check2\n",
      "52\n",
      "check1\n",
      "check2\n",
      "53\n",
      "check1\n",
      "check2\n",
      "54\n",
      "check1\n",
      "check2\n",
      "55\n",
      "check1\n",
      "check2\n",
      "56\n",
      "check1\n",
      "check2\n",
      "57\n",
      "check1\n",
      "check2\n",
      "58\n",
      "check1\n",
      "check2\n",
      "59\n",
      "check1\n",
      "check2\n",
      "60\n",
      "check1\n",
      "check2\n",
      "61\n",
      "check1\n",
      "check2\n",
      "62\n",
      "check1\n",
      "check2\n",
      "63\n",
      "check1\n",
      "check2\n",
      "64\n",
      "check1\n",
      "check2\n",
      "65\n",
      "check1\n",
      "check2\n",
      "66\n",
      "check1\n",
      "check2\n",
      "67\n",
      "check1\n",
      "check2\n",
      "68\n",
      "check1\n",
      "check2\n",
      "69\n",
      "check1\n",
      "check2\n",
      "70\n",
      "check1\n",
      "check2\n",
      "71\n",
      "check1\n",
      "check2\n",
      "72\n",
      "check1\n",
      "check2\n",
      "73\n",
      "check1\n",
      "check2\n",
      "74\n",
      "check1\n",
      "check2\n",
      "75\n",
      "check1\n",
      "check2\n",
      "76\n",
      "check1\n",
      "check2\n",
      "77\n",
      "check1\n",
      "check2\n",
      "78\n",
      "check1\n",
      "check2\n",
      "79\n",
      "check1\n",
      "check2\n",
      "80\n",
      "check1\n",
      "check2\n",
      "81\n",
      "check1\n",
      "check2\n",
      "82\n",
      "check1\n",
      "check2\n",
      "83\n",
      "check1\n",
      "check2\n",
      "84\n",
      "check1\n",
      "check2\n",
      "85\n",
      "check1\n",
      "check2\n",
      "86\n",
      "check1\n",
      "check2\n",
      "87\n",
      "check1\n",
      "check2\n",
      "88\n",
      "check1\n",
      "check2\n",
      "89\n",
      "check1\n",
      "check2\n",
      "90\n",
      "check1\n",
      "check2\n",
      "91\n",
      "check1\n",
      "check2\n",
      "92\n",
      "check1\n",
      "check2\n",
      "93\n",
      "check1\n",
      "check2\n",
      "94\n",
      "check1\n",
      "check2\n",
      "95\n",
      "check1\n",
      "check2\n",
      "96\n",
      "check1\n",
      "check2\n",
      "97\n",
      "check1\n",
      "check2\n",
      "98\n",
      "check1\n",
      "check2\n",
      "99\n",
      "check1\n",
      "check2\n",
      "100\n",
      "check1\n",
      "check2\n",
      "prepare to stack\n",
      "(1997, 100)\n"
     ]
    }
   ],
   "source": [
    "#loop  start om 19u10\n",
    "mydir = \"C:/Users/celie/OneDrive - ugentbe/NOG NODIG/2de master/Masterproef/Datasets/All_results/\"\n",
    "\n",
    "c1 = np.repeat(\"leavespaceforaword\", 38200) #should have 38178 rows, but taking 38200 just to be save\n",
    "alfa = np.column_stack([c1, c1,c1,c1,c1,c1,c1,c1,c1,c1])\n",
    "\n",
    "c2 = np.repeat(\"leavespaceforaword\", 3000) #should have 2828 rows, but taking 2840 just to be save \n",
    "peak_ar = np.column_stack([c2, c2,c2,c2,c2,c2,c2,c2])\n",
    "\n",
    "\n",
    "c3 = np.repeat(\"\", 1)\n",
    "c4 = np.repeat(\"\",1)\n",
    "\n",
    "ROI_base = numpy.zeros(shape=(1,2))\n",
    "ROI_ar = np.column_stack([c3, c4, ROI_base])\n",
    "ROI_tot= np.column_stack([c3, c4, ROI_base])\n",
    "\n",
    "\n",
    "varkeep_8 = {}\n",
    "varkeep_9 = {}\n",
    "varkeep_10 = {}\n",
    "varkeep_11= {}\n",
    "varkeep_alfa= {}\n",
    "\n",
    "\n",
    "c1 = np.repeat(\"\", 1)\n",
    "c2 = np.repeat(\"\",1)\n",
    "\n",
    "ROI_base = numpy.zeros(shape=(1,2))\n",
    "ROI_ar = np.column_stack([c1, c2, ROI_base])\n",
    "ROI_tot= np.column_stack([c1, c2, ROI_base])\n",
    "\n",
    "power_arr = numpy.zeros(shape = (1,5))\n",
    "power_arr_right =numpy.zeros(shape = (1,5))\n",
    "power_arr_left = numpy.zeros(shape = (1,5))\n",
    "tot_power_arr = numpy.zeros(shape = (1,5))\n",
    "tot_power_arr_right =numpy.zeros(shape = (1,5))\n",
    "tot_power_arr_left = numpy.zeros(shape = (1,5))\n",
    "\n",
    "varkeep = {}\n",
    "varhigh = {}\n",
    "varall = {}\n",
    "TSkeep = {}\n",
    "leftDLPFC_keep = {}\n",
    "rightDLPFC_keep = {}\n",
    "\n",
    "indices = [7,  13, 15, 16, 17,18, 19, 20, 21, 30, 31, 33,35, 36, 45, 51, 53, 54, 55, 56, 57, 58, 59, 68, 69, 71 , 73,74]\n",
    "counter = 0\n",
    "loop = 1\n",
    "peakcount = 0\n",
    "\n",
    "NS_arr = np.arange(0,26, 0.25)\n",
    "\n",
    "win = 4 * sf\n",
    "sf = 1024\n",
    "\n",
    "freq_range = [8,12] \n",
    "\n",
    "nsubs = 101\n",
    "for i in range(nsubs):\n",
    "    if i == 0:\n",
    "        pass \n",
    "    else:\n",
    "        value = 10**-6\n",
    "        print(loop)\n",
    "        \n",
    "        NS = NS_arr[i] \n",
    "        \n",
    "        #create simulator \n",
    "        sim = simulator.Simulator(model = oscilator, connectivity = white_matter,\n",
    "                          coupling = white_matter_coupling, \n",
    "                          integrator = integrators.EulerStochastic(dt=0.01220703125, noise=noise.Additive(noise_seed = int(NS), nsig=numpy.array([value]))),\n",
    "                        monitors =  what_to_watch, stimulus = stimulus)\n",
    "\n",
    "        sim.configure()\n",
    "\n",
    "        print(\"check1\")\n",
    "        \n",
    "        #Perform the simulation\n",
    "        tavg_data = []\n",
    "        tavg_time = []\n",
    "\n",
    "        for raw, tavg in sim(simulation_length=2500):\n",
    "    \n",
    "            if not tavg is None:\n",
    "                tavg_time.append(tavg[0])\n",
    "                tavg_data.append(tavg[1])\n",
    "    \n",
    "        #Make the lists numpy.arrays for easier use.\n",
    "        RAW = numpy.array(tavg_data)\n",
    "        RAW_real = RAW[:, 0, :, 0]\n",
    "        raw_time = tavg_time\n",
    "        \n",
    "        #delete the first 500ms\n",
    "        list_deleterows = [*range(0,500,1)]\n",
    "        RAW_real = np.delete(RAW_real, list_deleterows, 0)\n",
    "        raw_time = np.delete(raw_time, list_deleterows, 0)\n",
    "        \n",
    "        print(\"check2\")\n",
    "        \n",
    "        #save average time series (across regions)\n",
    "        AV = RAW_real\n",
    "        AV_ar = AV.mean(axis = 1) \n",
    "        TSkeep[\"data\" + str(i)] = AV_ar\n",
    "        \n",
    "        #save time series of left DLPFC (for alpha asymmetry later)\n",
    "        leftDLPFC = RAW_real[:,56]\n",
    "        leftDLPFC_keep[\"data\" + str(i)] = leftDLPFC\n",
    "\n",
    "        #save time series of right DLPFC (for alpha asymmetry later)\n",
    "        rightDLPFC = RAW_real[:,18]\n",
    "        rightDLPFC_keep[\"data\"+str(i)] = rightDLPFC\n",
    "        \n",
    "        #store PSDs\n",
    "        #psd for all brain \n",
    "        for x in range(5):            \n",
    "            low, high = 8+x, 9+x\n",
    "            if x == 4:\n",
    "                low, high = 8, 12\n",
    "            \n",
    "            power_arr[0][x] = powercalc(AV_ar, low, high, sf, win)\n",
    "            power_arr_left[0][x] = powercalc(leftDLPFC, low, high, sf, win)\n",
    "            power_arr_right[0][x] = powercalc(rightDLPFC, low, high, sf, win)\n",
    "            \n",
    "        if loop == 1:\n",
    "            tot_power_arr = numpy.copy(power_arr)\n",
    "            tot_power_arr_left = numpy.copy(power_arr_left)\n",
    "            tot_power_arr_right = numpy.copy(power_arr_right)\n",
    "        else:\n",
    "            tuplePower = (tot_power_arr, power_arr)\n",
    "            tot_power_arr = np.vstack(tuplePower)\n",
    "                \n",
    "            tuplePower_left = (tot_power_arr_left, power_arr_left)\n",
    "            tot_power_arr_left = np.vstack(tuplePower_left)\n",
    "                \n",
    "            tuplePower_right = (tot_power_arr_right, power_arr_right)\n",
    "            tot_power_arr_right = np.vstack(tuplePower_right)\n",
    "        \n",
    "        counter_coh = 0\n",
    "        for ROI1 in indices:\n",
    "            #save power spectrum of left DLPFC\n",
    "            ROIdata = RAW_real[:,ROI1]\n",
    "        \n",
    "            freqs_roi, psd_roi = signal.welch(ROIdata, sf, nperseg = win)\n",
    "        \n",
    "            fm = FOOOF(peak_width_limits=[1, 8])\n",
    "            fm.fit(freqs_roi, psd_roi, freq_range)\n",
    "            if not fm.has_model:\n",
    "                print(\"model fitting failed\")\n",
    "                Peak = \"Failed\"\n",
    "                Power = \"Failed\"\n",
    "                Peak_width = \"Failed\"\n",
    "                Amplitude = \"Failed\"\n",
    "            else:\n",
    "                peaks_roi = fm.get_params('peak_params')\n",
    "        \n",
    "                if check_nan(peaks_roi) == True:\n",
    "                    Peak = \"none\"\n",
    "                    Power = \"none\"\n",
    "                    Peak_width = \"none\"\n",
    "                    Amplitude = \"none\"\n",
    "                else:\n",
    "                    Peak = peaks_roi[0][0]\n",
    "                    Power = peaks_roi[0][1]\n",
    "                    Peak_width = peaks_roi[0][2]\n",
    "                    Amplitude = Power**2\n",
    "                \n",
    "            peak_ar[peakcount][0] = i\n",
    "            peak_ar[peakcount][1] = \"depressed\"\n",
    "            peak_ar[peakcount][2] = \"LF\"\n",
    "            peak_ar[peakcount][3] = str(regions[ROI1])\n",
    "            peak_ar[peakcount][4] = Peak\n",
    "            peak_ar[peakcount][5] = Power\n",
    "            peak_ar[peakcount][6] = Amplitude\n",
    "            peak_ar[peakcount][7] = Peak_width\n",
    "            \n",
    "            \n",
    "            peakcount = peakcount + 1\n",
    "            \n",
    "            for ROI2 in indices:\n",
    "                if ROI1 == ROI2:\n",
    "                    pass\n",
    "                elif ROI1 > ROI2:\n",
    "                    pass\n",
    "                else:\n",
    "                    x = RAW_real[:,ROI1]\n",
    "                    y = RAW_real[:,ROI2]\n",
    "                    \n",
    "                    cxy, f = cohere(x, y, 256, 1. / dt)\n",
    "                    cxy_alfa, f_alfa = cxy[21:32], f[21:32] #8-12Hz\n",
    "                    cxy_8, f_8 = cxy[21:24], f[21:24] #band 8-9Hz\n",
    "                    cxy_9, f_9 = cxy[24:26], f[24:26] # 9-10Hz\n",
    "                    cxy_10, f_10 = cxy[26:29], f[26:29] #10-11Hz\n",
    "                    cxy_11, f11 = cxy[29:32], f[29:32] #11-12Hz\n",
    "                    cxy_alfa, f_alfa = cxy[21:32], f[21:32] #8-12Hz\n",
    "                    \n",
    "                    alfa[counter][0] = i #subject id\n",
    "                    alfa[counter][1] = \"depressed\"\n",
    "                    alfa[counter][2] = \"LF\"\n",
    "                    alfa[counter][3] = str(regions[ROI1])\n",
    "                    alfa[counter][4] = str(regions[ROI2])\n",
    "                    alfa[counter][5] = mean(cxy_alfa)\n",
    "                    alfa[counter][6] = mean(cxy_8)\n",
    "                    alfa[counter][7] = mean(cxy_9)\n",
    "                    alfa[counter][8] = mean(cxy_10)\n",
    "                    alfa[counter][9] = mean(cxy_11)\n",
    "                    \n",
    "                                        #make matrix of coherence measures\n",
    "                    coh_8 = cxy_8\n",
    "                    coh_9 = cxy_9\n",
    "                    coh_10 = cxy_10\n",
    "                    coh_11= cxy_11\n",
    "                    coh_alfa= cxy_alfa\n",
    "                        \n",
    "                    if counter_coh == 0:\n",
    "                        coh_tot_8 = coh_8\n",
    "                        coh_tot_9 = coh_9\n",
    "                        coh_tot_10 = coh_10\n",
    "                        coh_tot_11= coh_11\n",
    "                        coh_tot_alfa = coh_alfa\n",
    "                        \n",
    "                        if loop == 1:\n",
    "                            ROI_tot[0][0] = str(regions[ROI1])\n",
    "                            ROI_tot[0][1] = str(regions[ROI2])\n",
    "                            ROI_tot[0][2] = ROI1\n",
    "                            ROI_tot[0][3] = ROI2\n",
    "            \n",
    "\n",
    "                    else:\n",
    "                        tuplex = (coh_tot_8, coh_8)\n",
    "                        coh_tot_8 = np.vstack(tuplex)\n",
    "                        \n",
    "                        tuplex = (coh_tot_9, coh_9)\n",
    "                        coh_tot_9 = np.vstack(tuplex)\n",
    "                        \n",
    "                        tuplex = (coh_tot_10, coh_10)\n",
    "                        coh_tot_10 = np.vstack(tuplex)\n",
    "                        \n",
    "                        tuplex = (coh_tot_11, coh_11)\n",
    "                        coh_tot_11 = np.vstack(tuplex)\n",
    "                        \n",
    "                        \n",
    "                        tuplex = (coh_tot_alfa, coh_alfa)\n",
    "                        coh_tot_alfa = np.vstack(tuplex)\n",
    "                        \n",
    "                        if loop == 1:\n",
    "                            ROI_ar[0][0] = str(regions[ROI1])\n",
    "                            ROI_ar[0][1] = str(regions[ROI2])\n",
    "                            ROI_ar[0][2] = ROI1\n",
    "                            ROI_ar[0][3] = ROI2\n",
    "                            tupleROI = (ROI_tot, ROI_ar)\n",
    "                            ROI_tot = np.vstack(tupleROI)\n",
    "                            \n",
    "                    counter_coh = counter_coh + 1\n",
    "                    \n",
    "                    counter = counter + 1\n",
    "    \n",
    "            \n",
    "        \n",
    "        #load in depressed brains \n",
    "        varkeep_8[\"data\" + str(i)] = coh_tot_8\n",
    "        varkeep_9[\"data\" + str(i)] = coh_tot_9\n",
    "        varkeep_10[\"data\" + str(i)] = coh_tot_10\n",
    "        varkeep_11[\"data\" + str(i)] = coh_tot_11\n",
    "        varkeep_alfa[\"data\" + str(i)] = coh_tot_alfa\n",
    "        \n",
    "        loop = loop + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "print('prepare to stack')\n",
    "\n",
    "file_name = mydir + \"/LF_coherence.csv\"\n",
    "outcome = pd.DataFrame.from_records(alfa)\n",
    "outcome.columns = [\"Subject\", \"Group\", \"Treatment\", \"ROI1\", \"ROI2\", \"whole band\", \"8-9Hz\",\"9-10Hz\",\"10-11Hz\",\"11-12Hz\"]\n",
    "outcome.to_csv(path_or_buf = file_name, index = False)\n",
    "\n",
    "file_name = mydir + \"/LF_peak.csv\"\n",
    "outcome = pd.DataFrame.from_records(peak_ar)\n",
    "outcome.columns = [\"Subject\", \"Group\", \"Treatment\", \"ROI1\", \"frequency\", \"power\", \"amplitude\", \"Bandwith peak\"]\n",
    "outcome.to_csv(path_or_buf = file_name, index = False)\n",
    "\n",
    "#coherence 8_9Hz\n",
    "namelist = []\n",
    "for i in range(nsubs):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name = varkeep_8[\"data\" + str(i)] \n",
    "        namelist.append(name)\n",
    "\n",
    "nametup = tuple(namelist)\n",
    "array_8 = np.hstack(nametup)\n",
    "        \n",
    "file_name = mydir + \"/LF_coh_total_hstacked_8\" + str(i) + \".h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_8)\n",
    "h5f.close()\n",
    "\n",
    "#coherence 9_10Hz\n",
    "namelist = []\n",
    "for i in range(nsubs):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name = varkeep_9[\"data\" + str(i)] \n",
    "        namelist.append(name)\n",
    "\n",
    "nametup = tuple(namelist)\n",
    "array_9 = np.hstack(nametup)\n",
    "        \n",
    "file_name = mydir + \"/LF_coh_total_hstacked_9\" + str(i) + \".h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_9)\n",
    "h5f.close()\n",
    "\n",
    "#coherence 10-11Hz\n",
    "namelist = []\n",
    "for i in range(nsubs):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name = varkeep_10[\"data\" + str(i)] \n",
    "        namelist.append(name)\n",
    "\n",
    "nametup = tuple(namelist)\n",
    "array_10 = np.hstack(nametup)\n",
    "        \n",
    "file_name = mydir + \"/LF_coh_total_hstacked_10\" + str(i) + \".h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_10)\n",
    "h5f.close()\n",
    "\n",
    "#coherence 11-12Hz\n",
    "namelist = []\n",
    "for i in range(nsubs):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name = varkeep_11[\"data\" + str(i)] \n",
    "        namelist.append(name)\n",
    "\n",
    "nametup = tuple(namelist)\n",
    "array_11 = np.hstack(nametup)\n",
    "        \n",
    "file_name = mydir + \"/LF_coh_total_hstacked_11\" + str(i) + \".h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_11)\n",
    "h5f.close()\n",
    "\n",
    "#coherence 8-12\n",
    "namelist = []\n",
    "for i in range(nsubs):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name = varkeep_alfa[\"data\" + str(i)] \n",
    "        namelist.append(name)\n",
    "\n",
    "nametup = tuple(namelist)\n",
    "array_alfa = np.hstack(nametup)\n",
    "        \n",
    "file_name = mydir + \"/LF_coh_total_hstacked_alfa\" + str(i) + \".h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_alfa)\n",
    "h5f.close()\n",
    "\n",
    "\n",
    "#store order of pairs \n",
    "\n",
    "file_name = mydir + \"/LF_ROI_order.csv\"\n",
    "outcome = pd.DataFrame.from_records(ROI_tot)\n",
    "outcome.columns = [\"ROI1\", \"ROI2\", \"ROI1_nr\", \"ROI2_nr\"]\n",
    "outcome.to_csv(path_or_buf = file_name, index = False)\n",
    "\n",
    "\n",
    "        \n",
    "file_name = mydir + \"/LF_Time.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('time', data=raw_time)\n",
    "h5f.close()\n",
    "\n",
    "\n",
    "f_array  = f[21:32]\n",
    "file_name = mydir + \"/LF_Frequencies.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=f_array)\n",
    "h5f.close()\n",
    "\n",
    "#compute complete array containing all time series of all 100simulations   \n",
    "namelist4 = []\n",
    "for i in range(nsubs): #range(101):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name4 = TSkeep[\"data\" + str(i)]\n",
    "        namelist4.append(name4)\n",
    "\n",
    "nametup4 = tuple(namelist4)\n",
    "array_TS = np.column_stack(nametup4)\n",
    "\n",
    "print(shape(array_TS))\n",
    "\n",
    "#make dataset containing all 100 right DLPFC time series + average across them\n",
    "namelist5 = []\n",
    "for i in range(nsubs): #range(101):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name5 = rightDLPFC_keep[\"data\"+str(i)] \n",
    "        namelist5.append(name5)\n",
    "\n",
    "nametup5 = tuple(namelist5)\n",
    "array_rDLPFC = np.column_stack(nametup5)\n",
    "\n",
    "\n",
    "#make dataset containing all 100 left DLPFC time series \n",
    "namelist6 = []\n",
    "for i in range(nsubs): #range(101):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        name6 = leftDLPFC_keep[\"data\"+str(i)] \n",
    "        namelist6.append(name6)\n",
    "\n",
    "nametup6 = tuple(namelist6)\n",
    "array_lDLPFC = np.column_stack(nametup6)\n",
    "\n",
    "file_name = mydir + \"/LF_power_alpha_overall.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=tot_power_arr)\n",
    "h5f.close()\n",
    "\n",
    "file_name = mydir + \"/LF_power_alpha_rDLPFC.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=tot_power_arr_right)\n",
    "h5f.close()\n",
    "\n",
    "file_name = mydir + \"/LF_power_alpha_lDLPFC.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=tot_power_arr_left)\n",
    "h5f.close()\n",
    "\n",
    "file_name = mydir + \"/LF_rightDLPFC.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_rDLPFC)\n",
    "h5f.close()\n",
    "\n",
    "file_name = mydir + \"/LF_leftDLPFC.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('data', data=array_lDLPFC)\n",
    "h5f.close()\n",
    "        \n",
    "file_name = mydir + \"/LF_Time.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('time', data=raw_time)\n",
    "h5f.close()\n",
    "\n",
    "file_name = mydir + \"/LF_TimeSeries.h5\"\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('time', data=array_TS )\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#started 19u06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "thesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
